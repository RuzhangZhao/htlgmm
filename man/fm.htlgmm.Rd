% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/htlgmm.R
\name{fm.htlgmm}
\alias{fm.htlgmm}
\title{Fine-mapping for htlgmm.}
\usage{
fm.htlgmm(
  y,
  Z,
  W = NULL,
  study_info = NULL,
  A = "default",
  penalty_type = "adaptivelasso",
  family = "gaussian",
  initial_with_type = "ridge",
  beta_initial = NULL,
  hat_thetaA = NULL,
  V_thetaA = NULL,
  use_offset = FALSE,
  V_thetaA_sandwich = FALSE,
  remove_penalty_Z = FALSE,
  remove_penalty_W = FALSE,
  inference = TRUE,
  refine_C = FALSE,
  sqrt_matrix = "cholesky",
  use_cv = TRUE,
  type_measure = "default",
  nfolds = 10,
  fix_lambda = NULL,
  lambda_list = NULL,
  nlambda = 100,
  lambda.min.ratio = 1e-04,
  tune_ratio = FALSE,
  fix_ratio = NULL,
  ratio_list = NULL,
  gamma_adaptivelasso = 1/2,
  use_sparseC = FALSE,
  seed.use = 97
)
}
\arguments{
\item{y}{The variable of interest, which can be continuous or binary.}

\item{Z}{The overlapping features in both main and external studies.}

\item{W}{The unmatched features only in main study, the default is NULL.}

\item{study_info}{The trained model from external study, including estimate coefficients, estimated variance-covariance matrix and sample size.
The 'study_info' is in the format of list. The first item is 'Coeff', the second iterm is 'Covariance', and the third item is 'Sample_size'.}

\item{A}{The covariates for study-specific adjustment. The default is 'default', which is 'NULL' for 'gaussian' family, '1' for 'binomial' family.
Other than c('default',NULL,1), A must be a matrix whose dimension is the same as the sample dimension or Z and W.
For continuous variable, we suggest scaling the features Z, W to eliminate intercept term.  If 'A = NULL', there is no intercept term included.
For binary variable, we use intercept term by 'A=1' to adjust for different binary trait ratios in main and external studies.
If there is only intercept term in A, we use 'A=1'.
A are the features working for adjustment in reduced model, but A is not summarized in summary statistics(input:study_info).}

\item{penalty_type}{The penalty type for htlgmm, chosen from c("none","lasso","adaptivelasso","ridge"). The default is "adaptivelasso".
If 'penalty_type = 'none' ', we use without penalty. (For continuous y, we use ordinary least square, and for binary y, we use logistic regression without penalty.)}

\item{family}{The family is chosen from c("gaussian","binomial"). Linear regression for "gaussian" and logistic regression for "binomial".}

\item{initial_with_type}{Get initial estimation for beta using main study data only
by cross validation using penalty regression, chosen from c("ridge","lasso","glm"). The default is "ridge". If penalty_type = 'glm',
for continuous y, we use ordinary least square, and for binary y, we use logistic regression without penalty.)}

\item{beta_initial}{The initial estimation for beta if a consistent estimator is available.
E.g., one may input htlgmm result as beta_initial for more rounds to refine the final estimation.
The default is NULL, and main study is used for initial estimation according to 'initial_with_type'.}

\item{hat_thetaA}{If A is not NULL, one can provide hat_thetaA as the input. If 'hat_thetaA = NULL', we estimate hat_thetaA with glm by main study.}

\item{V_thetaA}{If A is not NULL, one can provide V_thetaA as the input. If 'V_thetaA = NULL', we estimate V_thetaA with glm by main study.}

\item{use_offset}{Whether to use offset regarding the external model estimated coefficient. The default is FALSE.}

\item{V_thetaA_sandwich}{Whether to apply sandwich formula to compute the variance-covariance matrix if hat_thetaA.The default is FALSE.}

\item{remove_penalty_Z}{Not penalize Z if it is TRUE. The default is FALSE.}

\item{remove_penalty_W}{Not penalize W if it is TRUE. The default is FALSE.}

\item{inference}{Whether to do inference without penalty or post-selection inference with adaptive lasso penalty. The default is TRUE.}

\item{refine_C}{When computing the variance, whether recompute the weighting matrix C using final estimated beta.}

\item{sqrt_matrix}{The method to split weighting matrix into square root matrix. Select from c('svd','cholesky'), where 'cholesky' generates faster computation.}

\item{use_cv}{Whether to use cross validation to determine the best lambda (or ratio).}

\item{type_measure}{Select from c("default", "mse", "deviance", "auc"). Default is mse(liner), deviance(logistic). 'auc' is another choice for binary y.}

\item{nfolds}{The fold number for cross validation. Only work for use_cv = TRUE.The default is 10.}

\item{fix_lambda}{Without cross validation, fix the lambda. The default is NULL.}

\item{lambda_list}{Customize the input lambda list for validation. The default is NULL to generate lambda list according to glmnet.}

\item{nlambda}{The number of lambda values - default is 100.}

\item{lambda.min.ratio}{Smallest value for lambda, as a fraction of lambda.max. The default is 0.0001.}

\item{tune_ratio}{Whether to use two-lambda stratgey. The default is FALSE. This is not applied to coxph model.}

\item{fix_ratio}{The fixed ratio for two-lambda strategy. The ratio is multiplied for Z features. The default is NULL. If it is NULL, select the best ratio via cross validation.}

\item{ratio_list}{The ratio list if it is preset. The default is NULL and ratio list will be generated.}

\item{gamma_adaptivelasso}{The gamma for adaptive lasso. Select from c(1/2,1,2). The default is 1/2.}

\item{use_sparseC}{Whether to use approximate version of weighting matrix C.
If approximation, use the diagonal of inverse of C(inv_C) to approximate the inv_C. The default is TRUE.
When main study sample size is limited, use_sparseC = TRUE is recommended.
When main study sample size is large enough, use_sparseC = FALSE is recommended.}

\item{seed.use}{The seed for  97.}
}
\value{
\itemize{
 \item{beta:} The target coefficient estimation, the features will go in the order of (A,Z,W).
 \item{lambda_list:} The lambda list for cross validation.
 \item{ratio_list:} The ratio list for validation (cross validation or holdout validation).
 \item{fix_lambda:} If the fix_lambda is not null, we output fix_lambda.
 \item{fix_ratio:} If the fix_ratio is not null, we output fix_ratio.
 \item{lambda_min:} The selected best lambda by cross validation.
 \item{ratio_min:} The selected best ratio by cross validation.
 \item{cv_mse:} The mean square error(mse) when family = "gaussian", and use_cv = TRUE.
 \item{cv_dev:} The deviance(dev) when family = "binomial", and use_cv = TRUE.
 \item{cv_auc:} The area under the curve of sensitivity specificity when family = "binomial", and use_cv = TRUE.
 \item{selected_vars:} For inference or post-selection inference, we output the inference results by a list. \itemize{
 \item{position:} The index of nonzero positions, the index comes from X = (A,Z,W).
 \item{name:} The feature name of nonzero positions. If there is no default name, we name it after Ai, Zi, Wi.
 \item{coef:} The coefficients of nonzero positions.
 \item{variance:} The variances for features with glm inference, for selected features with post-selection inference.
 \item{pval:} For p values for nonzero positions.
 \item{FDR_adjust_position:} The FDR adjusted positions passing significant level 0.05 after BH adjustment (Benjamini & Hochberg).
 \item{FDR_adjust_name:} The feature name based on FDR_adjust_position.
 }
 }
}
\description{
fm.htlgmm fits a generalized linear model via penalized generalized method of moments,
i.e. Heterogeneous Transfer Learning via Generalized Method of Moments,
with special focus on fine-mapping, where fm stands for fine-mapping.
The input requires main study and GWAS external studies.
The fm.htlgmm support cross-validation.
}
\details{
Fine-mapping for htlgmm.
}
