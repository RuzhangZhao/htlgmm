% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/htlgmm.R
\name{group.fm.htlgmm}
\alias{group.fm.htlgmm}
\title{Fine-mapping for htlgmm with group.}
\usage{
group.fm.htlgmm(
  y,
  Z,
  W = NULL,
  study_info = NULL,
  A = "default",
  penalty_type = "adaptivelasso",
  family = "gaussian",
  decor_method = "pca",
  max_cor = 0.9,
  min_cor = 0.5,
  ncor = 5,
  ...
)
}
\arguments{
\item{y}{The variable of interest, which can be continuous or binary.}

\item{Z}{The overlapping features in both main and external studies.}

\item{W}{The unmatched features only in main study, the default is NULL.}

\item{study_info}{The trained model from external study, including estimate coefficients, estimated variance-covariance matrix and sample size.
The 'study_info' is in the format of list. The first item is 'Coeff', the second iterm is 'Covariance', and the third item is 'Sample_size'.}

\item{A}{The covariates for study-specific adjustment. The default is 'default', which is 'NULL' for 'gaussian' family, '1' for 'binomial' family.
Other than c('default',NULL,1), A must be a matrix whose dimension is the same as the sample dimension or Z and W.
For continuous variable, we suggest scaling the features Z, W to eliminate intercept term.  If 'A = NULL', there is no intercept term included.
For binary variable, we use intercept term by 'A=1' to adjust for different binary trait ratios in main and external studies.
If there is only intercept term in A, we use 'A=1'.
A are the features working for adjustment in reduced model, but A is not summarized in summary statistics(input:study_info).}

\item{penalty_type}{The penalty type for htlgmm, chosen from c("none","lasso","adaptivelasso","ridge"). The default is "adaptivelasso".
If 'penalty_type = 'none' ', we use without penalty. (For continuous y, we use ordinary least square, and for binary y, we use logistic regression without penalty.)}

\item{family}{The family is chosen from c("gaussian","binomial"). Linear regression for "gaussian" and logistic regression for "binomial".}

\item{decor_method}{The method to decorrelate the variables. The default is "pca".}

\item{max_cor}{The maximum correlation used to group variables. The default is 0.9.}

\item{min_cor}{The minimum correlation used to group variables. The default is 0.5}

\item{ncor}{The length of explored correlation list. The default is 5.}

\item{...}{Check the input list for fm.htlgmm.}
}
\value{
\itemize{
 \item{beta:} The target coefficient estimation, the features will go in the order of (A,Z,W).
 \item{lambda_list:} The lambda list for cross validation.
 \item{ratio_list:} The ratio list for validation (cross validation or holdout validation).
 \item{fix_lambda:} If the fix_lambda is not null, we output fix_lambda.
 \item{fix_ratio:} If the fix_ratio is not null, we output fix_ratio.
 \item{lambda_min:} The selected best lambda by cross validation.
 \item{ratio_min:} The selected best ratio by cross validation.
 \item{cv_mse:} The mean square error(mse) when family = "gaussian", and use_cv = TRUE.
 \item{cv_dev:} The deviance(dev) when family = "binomial", and use_cv = TRUE.
 \item{cv_auc:} The area under the curve of sensitivity specificity when family = "binomial", and use_cv = TRUE.
 \item{selected_vars:} For inference or post-selection inference, we output the inference results by a list. \itemize{
 \item{position:} The index of nonzero positions, the index comes from X = (A,Z,W).
 \item{name:} The feature name of nonzero positions. If there is no default name, we name it after Ai, Zi, Wi.
 \item{coef:} The coefficients of nonzero positions.
 \item{variance:} The variances for features with glm inference, for selected features with post-selection inference.
 \item{pval:} For p values for nonzero positions.
 \item{FDR_adjust_position:} The FDR adjusted positions passing significant level 0.05 after BH adjustment (Benjamini & Hochberg).
 \item{FDR_adjust_name:} The feature name based on FDR_adjust_position.
 }
 \item{group_list:} The group of variables to be assigned together.
 }
}
\description{
group.fm.htlgmm fits a generalized linear model via penalized generalized method of moments,
i.e. Heterogeneous Transfer Learning via Generalized Method of Moments,
with special focus on fine-mapping, where fm stands for fine-mapping.
The input requires main study and GWAS external studies.
The htlgmm.fm.group support cross-validation.
}
\details{
Group level fine-mapping for htlgmm.
}
