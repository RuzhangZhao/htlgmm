% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/intgmm.R
\name{intgmm}
\alias{intgmm}
\title{intgmm: integration with generalized method of moments(gmm).}
\usage{
intgmm(
  y,
  X,
  A = NULL,
  study_info = NULL,
  summary_type = "multi",
  family = "gaussian",
  G = 1,
  penalty_type = "lasso",
  initial_with_type = "ridge",
  beta_initial = NULL,
  remove_penalty_X = FALSE,
  remove_penalty_A = FALSE,
  lambda = 0,
  ratio = NULL,
  gamma_adaptivelasso = 1/2,
  inference = FALSE,
  use_sparseC = FALSE
)
}
\arguments{
\item{y}{The y for response variable, which can be continouse or binary.}

\item{X}{The matched features for internal and external data.}

\item{A}{The additional features only in internal data, the default is NULL.}

\item{study_info}{The summary statistics for X only from external data,
which can be summarized in the type of multivariate version or univariate version.}

\item{summary_type}{The summary statistics type, chosen from c("multi","uni"), where the default is "multi".}

\item{family}{The family is chosen from c("gaussian","binomial"). Linear regression for "gaussian" and logistic regression for "binomial".}

\item{G}{Usually used in "binomial" family, e.g. the intercept term for logistic regression, where the default is 1.
Usually not used in "gaussian" family.
G are the features working for adjustment in reduced model, but G is not summarized in summary statistics(input:study_info).}

\item{penalty_type}{The penalty type for intgmm, chosen from c("lasso","adaptivelasso","ridge"). The default is "lasso".}

\item{initial_with_type}{Get initial estimation for beta using internal data only
by cross validation using penalty regression, chosen from c("ridge","lasso"). The default is "ridge".}

\item{beta_initial}{The initial estimation for beta if a consistent estimator is available.
E.g., one may input intgmm result as beta_initial for more rounds to refine the final estimation.
The default is NULL, and internal data is used for initial estimation.}

\item{remove_penalty_X}{Not penalize X if it is TRUE. The default is FALSE.}

\item{remove_penalty_A}{Not penalize A if it is TRUE. The default is FALSE.}

\item{lambda}{Without cross validation, fix the lambda. The default is NULL.}

\item{ratio}{The fixed ratio of X for bi-lambda strategy. The default is NULL. If it is NULL, select the best ratio via cross validation or holdout validation.}

\item{gamma_adaptivelasso}{The gamma for adaptive lasso. Select from c(1/2,1,2). The default is 1/2.}

\item{inference}{Whether to do post-selection inference, only work for adaptivelasso. The default is FALSE.}

\item{use_sparseC}{Whether to use approximate version of weighting matrix C.
If approximation, use the diagonal of inverse of C(inv_C) to approximate the inv_C. The default is FALSE.
When internal data sample size is limited, use_sparseC = TRUE is recommended.
When internal data sample size is large enough, use_sparseC = FALSE is recommended.}
}
\value{
beta The target coefficient estimation.

fix_lambda The output lambda.

fix_ratio The output ratio.

corrected_pos For post-selection inference, they are the corrected position passing significant level 0.05 after BH adjustment (Benjamini & Hochberg).

nonzero_pos For estimated beta, the nonzero positions.

pval For nonzero_pos, the calculated p values.

nonzero_var For nonzero_pos, the calculated variances.
}
\description{
intgmm: integration with generalized method of moments(gmm).
}
\details{
intgmm: integration with generalized method of moments(gmm).
}
\examples{
set.seed(1)
X<-matrix(rnorm(18000),900,20)
A<-matrix(rnorm(2700),900,3)
X<-scale(X)
A<-scale(A)
coefXA<-c(rep(0,23))
coefXA[1:3]<-0.5
coefXA[21:22]<-0.5
internal_index<-1:100
external_index<-101:900
y<-cbind(X,A)\%*\%coefXA+rnorm(900,0,1)
#y_binary<-rbinom(n=900,size=1,prob=locfit::expit(cbind(X,A)\%*\%coefXA))
study_info_multi<-list()
reslm<-lm(y~.,data = data.frame(y=y[external_index],X[external_index,]))
study.m = list(Coeff=reslm$coefficients[-1],
               Covariance=vcov(reslm)[-1,-1],Sample_size=800)
study_info_multi[[1]] <- study.m
study_info_uni<-list()
for(i in 1:20){
reslm<-lm(y~.,data = data.frame(y=y[external_index],X[external_index,i]))
study.m = list(Coeff=reslm$coefficients[-1],
               Covariance=vcov(reslm)[-1,-1],Sample_size=800)
study_info_uni[[i]] <- study.m}

y<-scale(y,scale = FALSE)
library(glmnet)
res_glm<-glmnet(x=cbind(X[internal_index,],A[internal_index,]),y=y[internal_index],lambda=0)
res_intgmm_multi<-intgmm(y[internal_index],X[internal_index,],A[internal_index,],
    summary_type = "multi",study_info = study_info_multi,lambda=0,use_sparseC = TRUE)
res_intgmm_uni<-intgmm(y[internal_index],X[internal_index,],A[internal_index,],
    summary_type = "uni",study_info = study_info_uni,lambda=0,use_sparseC = TRUE)
    ee_lasso<-round(sum((coefXA-coef.glmnet(res_glm)[-1])^2),4)
ee_intgmm_lasso_multi<-round(sum((coefXA-res_intgmm_multi$beta)^2),4)
ee_intgmm_lasso_uni<-round(sum((coefXA-res_intgmm_uni$beta)^2),4)
print(paste0("Estimation Error: ","lasso(",ee_lasso,"); intgmm_lasso_multi(",
             ee_intgmm_lasso_multi,"); intgmm_lasso_uni(",ee_intgmm_lasso_uni,")"))


}
